{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunelaliyevaa/wildfire-detection-using-satellite-imagery/blob/main/input_image_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ovRmIxQc7S5C",
        "outputId": "0ddbc487-2118-4b1a-be18-73c833a6e8bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing completed successfully.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"preprocess_and_patch.py\n",
        "========================\n",
        "\n",
        "Apply *Contrast‑Limited Adaptive Histogram Equalisation* (CLAHE) to each\n",
        "image and slice it into fixed‑size patches. The script expects the input\n",
        "dataset to be organised as::\n",
        "\n",
        "    input_dir/\n",
        "        fire/\n",
        "        no_fire/\n",
        "\n",
        "and produces an identical folder tree in *output_dir* containing the\n",
        "generated patches::\n",
        "\n",
        "    output_dir/\n",
        "        fire/\n",
        "            <orig>_patch_1.png\n",
        "            <orig>_patch_2.png\n",
        "            ...\n",
        "        no_fire/\n",
        "            ...\n",
        "\n",
        "By default, every 512 × 512 image is divided into four 256 × 256 patches,\n",
        "but the patch size is configurable.\n",
        "\n",
        "Example\n",
        "-------\n",
        "```\n",
        "python preprocess_and_patch.py \\\n",
        "    --input-dir /data/labeled_images \\\n",
        "    --output-dir /data/patched_images \\\n",
        "    --patch-size 256 \\\n",
        "    --clip-limit 2.0 \\\n",
        "    --tile-size 8 8\n",
        "```\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Sequence\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Utility functions\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def create_clahe(clip_limit: float, tile_grid_size: tuple[int, int]) -> cv2.CLAHE:  # noqa: D401\n",
        "    \"\"\"Return an OpenCV CLAHE object with the given parameters.\"\"\"\n",
        "    return cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "\n",
        "\n",
        "def apply_clahe_rgb(image: np.ndarray, *, clahe: cv2.CLAHE) -> np.ndarray:\n",
        "    \"\"\"Apply CLAHE **per‑channel** in *Lab* colour‑space.\n",
        "\n",
        "    Args:\n",
        "        image: BGR ``numpy`` array as returned by ``cv2.imread``.\n",
        "        clahe: Pre‑configured ``cv2.CLAHE`` instance.\n",
        "    \"\"\"\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    l_eq = clahe.apply(l)\n",
        "    lab_eq = cv2.merge((l_eq, a, b))\n",
        "    return cv2.cvtColor(lab_eq, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "\n",
        "def split_into_patches(image: np.ndarray, *, patch_size: int = 256) -> list[np.ndarray]:\n",
        "    \"\"\"Return **non‑overlapping** ``patch_size`` square patches from *image*.\"\"\"\n",
        "    patches: list[np.ndarray] = []\n",
        "    h, w, _ = image.shape\n",
        "    for y in range(0, h, patch_size):\n",
        "        for x in range(0, w, patch_size):\n",
        "            patch = image[y : y + patch_size, x : x + patch_size]\n",
        "            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n",
        "                patches.append(patch)\n",
        "    return patches\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Core processing\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def process_dataset(\n",
        "    *,\n",
        "    input_dir: Path,\n",
        "    output_dir: Path,\n",
        "    categories: Sequence[str] = (\"fire\", \"no_fire\"),\n",
        "    patch_size: int = 256,\n",
        "    clip_limit: float = 2.0,\n",
        "    tile_grid: tuple[int, int] = (8, 8),\n",
        ") -> None:\n",
        "    \"\"\"Apply CLAHE + patching for every image in *input_dir*/*category*.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Root directory of original images.\n",
        "        output_dir: Root directory where patches are saved.\n",
        "        categories: Sub‑folders (classes) to process.\n",
        "        patch_size: Width / height of the square patches.\n",
        "        clip_limit: CLAHE clip limit.\n",
        "        tile_grid: CLAHE tile grid size as ``(rows, cols)``.\n",
        "    \"\"\"\n",
        "    clahe = create_clahe(clip_limit, tile_grid)\n",
        "\n",
        "    for category in categories:\n",
        "        src_folder = input_dir / category\n",
        "        dst_folder = output_dir / category\n",
        "        dst_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for img_name in src_folder.iterdir():\n",
        "            if not img_name.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
        "                continue\n",
        "            image = cv2.imread(str(img_name))\n",
        "            if image is None:\n",
        "                print(f\"Could not read {img_name}; skipping\")\n",
        "                continue\n",
        "\n",
        "            # --- Pre‑process & patch ---\n",
        "            image_eq = apply_clahe_rgb(image, clahe=clahe)\n",
        "            patches = split_into_patches(image_eq, patch_size=patch_size)\n",
        "\n",
        "            # --- Save patches ---\n",
        "            stem = img_name.stem\n",
        "            for i, patch in enumerate(patches, 1):\n",
        "                patch_filename = f\"{stem}_patch_{i}.png\"\n",
        "                cv2.imwrite(str(dst_folder / patch_filename), patch)\n",
        "\n",
        "    print(\"Processing completed successfully.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CLI interface\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def parse_args() -> argparse.Namespace:  # noqa: D401\n",
        "    \"\"\"Parse command‑line arguments.\"\"\"\n",
        "    p = argparse.ArgumentParser(description=\"CLAHE + patch generator\")\n",
        "    p.add_argument(\"--input-dir\", type=Path, required=True, help=\"Input dataset root\")\n",
        "    p.add_argument(\"--output-dir\", type=Path, required=True, help=\"Output root for patches\")\n",
        "    p.add_argument(\"--patch-size\", type=int, default=256, help=\"Square patch size (default: 256)\")\n",
        "    p.add_argument(\"--clip-limit\", type=float, default=2.0, help=\"CLAHE clip limit (default: 2.0)\")\n",
        "    p.add_argument(\"--tile-size\", type=int, nargs=2, metavar=(\"ROWS\", \"COLS\"), default=(8, 8), help=\"CLAHE tile grid size (default: 8 8)\")\n",
        "    p.add_argument(\"--categories\", nargs=\"+\", default=[\"fire\", \"no_fire\"], help=\"Category sub‑folders to process\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    process_dataset(\n",
        "        input_dir=args.input_dir,\n",
        "        output_dir=args.output_dir,\n",
        "        categories=args.categories,\n",
        "        patch_size=args.patch_size,\n",
        "        clip_limit=args.clip_limit,\n",
        "        tile_grid=tuple(args.tile_size),\n",
        "    )\n",
        "\"}\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A0Q-sP15LxhHrs7tyAYORGJ2HKkWv8bT",
      "authorship_tag": "ABX9TyPSQhIMlzMi6GjDab7Jd/wn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}