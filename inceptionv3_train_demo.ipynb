{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunelaliyevaa/wildfire-detection-using-satellite-imagery/blob/main/inceptionv3_train_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING DEMO NOTEBOOK**\n",
        "\n",
        "The general purpose of this notebook is to teach the user how to use the InceptionV3 model on a dataset to predict wildfire images in satellite imagery, as part of the user's wildfire detection project."
      ],
      "metadata": {
        "id": "wf2-eokifBWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOW TO SET UP**\n",
        "\n",
        " **↪** **Please clone the repository if anything needed to be downloaded:**"
      ],
      "metadata": {
        "id": "pQMAvj9lguQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gunelaliyevaa/wildfire-detection-using-satellite-imagery.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAM7VO6LhAS5",
        "outputId": "62f5cdfa-d943-4cd8-ac08-6a523642558d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wildfire-detection-using-satellite-imagery'...\n",
            "remote: Enumerating objects: 1013, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 1013 (delta 39), reused 47 (delta 36), pack-reused 945 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1013/1013), 203.09 MiB | 15.30 MiB/s, done.\n",
            "Resolving deltas: 100% (172/172), done.\n",
            "Updating files: 100% (678/678), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **Change the working directory to the actual project folder.**"
      ],
      "metadata": {
        "id": "6hJItYZPhDrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd wildfire-detection-using-satellite-imagery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtWFZhNshNo4",
        "outputId": "b80aafc4-1386-4790-c0ff-44af7b672028"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wildfire-detection-using-satellite-imagery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **Import libraries and install the custom library from the repository**"
      ],
      "metadata": {
        "id": "OZ2C0SZmhgv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "import lib as ml"
      ],
      "metadata": {
        "id": "N-lydMcRh45e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURATION section (optional)"
      ],
      "metadata": {
        "id": "YSw3x1OaZ1iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Section\n",
        "\n",
        "# Dataset Paths\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/sdp_wildfire_satellite_shared/azerbaijan_wildfire_imagery/kaggle_fire_dataset_sample/train_dataset\"\n",
        "TEST_DIR = \"/content/drive/MyDrive/sdp_wildfire_satellite_shared/azerbaijan_wildfire_imagery/kaggle_fire_dataset_sample/test_dataset\"\n",
        "\n",
        "# Image Parameters\n",
        "IMG_SIZE = (350, 350)  # Target size for resizing images\n",
        "BATCH_SIZE = 32        # Number of images processed in a batch\n",
        "\n",
        "# Model Training Parameters\n",
        "EPOCHS = 20            # Initial training epochs\n",
        "FINE_TUNE_EPOCHS = 10  # Additional epochs for fine-tuning\n",
        "LEARNING_RATE = 1e-3   # Initial learning rate\n",
        "FINE_TUNE_LR = 1e-5    # Lower learning rate for fine-tuning\n",
        "\n",
        "# Data Augmentation Settings\n",
        "AUGMENTATION_PARAMS = {\n",
        "    \"rescale\": 1.0 / 255,\n",
        "    \"rotation_range\": 20,\n",
        "    \"width_shift_range\": 0.2,\n",
        "    \"height_shift_range\": 0.2,\n",
        "    \"shear_range\": 0.2,\n",
        "    \"zoom_range\": 0.2,\n",
        "    \"horizontal_flip\": True,\n",
        "    \"validation_split\": 0.2\n",
        "}\n",
        "\n",
        "# Callbacks Settings\n",
        "EARLY_STOPPING_PATIENCE = 3  # Stop training if validation loss doesn't improve\n",
        "LR_REDUCE_PATIENCE = 2       # Reduce LR if no improvement in val_loss\n",
        "\n",
        "# Hardware Configuration\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ],
      "metadata": {
        "id": "z7lzfX2KaAo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATASET PATH, DATA BALANCE CHECK AND VISUALIZATION**"
      ],
      "metadata": {
        "id": "FbB1sEM9h7vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wildfire Dataset Information is given as:\n",
        "\n",
        "The dataset consists of satellite images extracted using an image extraction script\n",
        "from the Sentinel-2 image collection. These images have:\n",
        "- A resolution of 512x512 pixels\n",
        "- True-color representation (RGB)\n",
        "- PNG format"
      ],
      "metadata": {
        "id": "VFEWjxFBiIas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **We define paths to the wildfire dataset and the dataset contains two categories:**\n",
        "- 'train-wildfire' - Images showing wildfire scenes.\n",
        "- 'train-nowildfire' - Images with no wildfire present."
      ],
      "metadata": {
        "id": "pb65Cy_LinBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_path = '/content/drive/MyDrive/sdp_wildfire_satellite_shared/azerbaijan_wildfire_imagery/kaggle_fire_dataset_sample/train_dataset/train-wildfire'\n",
        "nofire_path = '/content/drive/MyDrive/sdp_wildfire_satellite_shared/azerbaijan_wildfire_imagery/kaggle_fire_dataset_sample/train_dataset/train-nowildfire'"
      ],
      "metadata": {
        "id": "vXXTgevbi28N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **We count the number of images in each category ensuring dataset is balanced since this is crucial for accuracy of the trained model. Printing results, we get count of each class.**"
      ],
      "metadata": {
        "id": "L7E2AgOsjO1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_images = len(os.listdir(fire_path))\n",
        "nofire_images = len(os.listdir(nofire_path))\n",
        "\n",
        "print(f\"Fire Images: {fire_images}\")\n",
        "print(f\"No Fire Images: {nofire_images}\")"
      ],
      "metadata": {
        "id": "jrikRG0Kjcgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **We can visualize a batch of fire and nofire images below**"
      ],
      "metadata": {
        "id": "i0YA9jzYq--c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images from the training generator\n",
        "fire_batch, fire_labels = next(train_generator)\n",
        "\n",
        "# Function to display a batch of images\n",
        "def visualize_batch(images, labels, class_names, title):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(9):  # Display 9 images\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(images[i])  # Images are already normalized\n",
        "        label = class_names[int(labels[i])]  # Convert label to class name\n",
        "        plt.title(label)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "# Class labels based on binary classification\n",
        "class_names = {0: \"No Fire\", 1: \"Fire\"}\n",
        "\n",
        "# Display batch of images\n",
        "visualize_batch(fire_batch, fire_labels, class_names, \"Sample Fire and No Fire Images\")\n"
      ],
      "metadata": {
        "id": "Yx_BHQf6q1oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPROCESSING**"
      ],
      "metadata": {
        "id": "Sq6xo7PCkF7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation & Preprocessing**\n",
        "\n",
        "**↪** **The dataset will be preprocessed and augmented using ImageDataGenerator.\n",
        "This helps improve model generalization by introducing slight variations in the images. Augmentations applied:**\n",
        "- Rescaling pixel values to the range [0,1] for normalization ( ❓ Neural networks perform better when input values are normalized. Rescaling helps speed up training and stabilizes weight updates.)\n",
        "- Random rotations (up to 20 degrees) ( ❓ Wildfire images may be captured from different angles due to satellite positioning. Rotations help the model learn orientation-invariant features.)\n",
        "- Horizontal shifting (up to 20% of image width) ( ❓ Fires may appear at different locations within an image. This shift prevents the model from over-relying on fixed spatial positions.)\n",
        "- Vertical shifting (up to 20% of image height) ( ❓ Similar to horizontal shifts, vertical shifts ensure the model does not depend on absolute positions of fire regions.)\n",
        "- Shearing transformations (up to 20%) ( ❓ Introduces a subtle distortion effect, mimicking real-world variations caused by different camera angles or satellite distortions.)\n",
        "- Zooming (up to 20%) ( ❓ Helps the model recognize fires at different scales, ensuring it can detect both small and large fire regions.)\n",
        "- Horizontal flipping ( ❓ Since fires can occur in any direction, flipping prevents the model from favoring a specific orientation.)\n",
        "- Splitting into training (80%) and validation (20%) sets ( ❓ Ensures the model is trained on one subset of the data while another unseen subset is used to evaluate its performance, preventing overfitting.)"
      ],
      "metadata": {
        "id": "OrUWG9qTm69P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,  # Normalize pixel values\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Split\n",
        ")"
      ],
      "metadata": {
        "id": "7oawsyeDnsN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOAD DATA FROM DIRECTORIES**\n"
      ],
      "metadata": {
        "id": "kKL3LUgmnxWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The generator loads images from the dataset directory, applies transformations,and prepares them for model training:**"
      ],
      "metadata": {
        "id": "U2_vum31rnYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/sdp_wildfire_satellite_shared/azerbaijan_wildfire_imagery/kaggle_fire_dataset_sample/train_dataset',\n",
        "    target_size=(350, 350),  # Resize all images to 350x350\n",
        "    batch_size=32,  # Process images in mini-batches of 32\n",
        "    class_mode='binary',  # Binary classification: wildfire vs. no wildfire\n",
        "    subset='training'  # Load only training images (80% of total)\n",
        ")"
      ],
      "metadata": {
        "id": "FWYr4T3yr1QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This generator loads the validation images (20% of the dataset) for model evaluation.**"
      ],
      "metadata": {
        "id": "9lf9hDa7sCdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_generator = datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/sdp_wildfire_satellite_shared/azerbaijan_wildfire_imagery/kaggle_fire_dataset_sample/train_dataset',\n",
        "    target_size=(350, 350),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "id": "oN8_C9j-sLO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A BIT ABOUT MODEL**"
      ],
      "metadata": {
        "id": "f4EaGtsftScI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The wildfire classification model is based on the InceptionV3 architecture,\n",
        "> a well-known deep convolutional neural network originally designed for image recognition.\n",
        "> InceptionV3 is pre-trained on ImageNet and fine-tuned on our wildfire dataset\n",
        "> to differentiate between wildfire and non-wildfire images.\n",
        "> The model is capable of extracting deep spatial features from images while  maintaining computational efficiency. It employs factorized convolutions and  auxiliary classifiers to improve learning performance and reduce overfitting.\n",
        "\n",
        ">**InceptionV3 Model Structure**\n",
        ">The InceptionV3 model is initialized with pre-trained weights from ImageNet and adapted for binary classification: wildfire vs. no wildfire.\n",
        "\n",
        ">The original fully connected layers are replaced with a custom classifier\n",
        ">The input image size is set to (350x350), consistent with the dataset\n",
        ">The model applies a global average pooling layer before classification\n",
        ">The final activation function is sigmoid, as this is a binary classification task\n",
        "\n",
        ">While the base InceptionV3 model is powerful, additional techniques such as\n",
        "data augmentation and transfer learning help improve performance.\n",
        ">Fine-tuning deeper layers of the model allows it to learn wildfire-specific patterns rather than generic image features.\n",
        "\n",
        ">The model is trained using binary cross-entropy loss and Adam optimizer,\n",
        "with real-time data augmentation applied to enhance generalization."
      ],
      "metadata": {
        "id": "egQM2d9Str8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture and Compilation**"
      ],
      "metadata": {
        "id": "Hqd4bZButyUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wildfire classification model is built using InceptionV3 as a feature extractor.\n",
        "\n",
        "This approach leverages the powerful convolutional layers of InceptionV3,\n",
        "which have been pre-trained on the large-scale ImageNet dataset.\n",
        "Instead of training from scratch, we use transfer learning by freezing\n",
        "the base layers and adding a custom classifier on top.\n",
        "This allows the model to retain general image features while focusing\n",
        "on wildfire-specific patterns in the dataset.\n",
        "\n",
        "**↪** **Load Pretrained InceptionV3 Model:**\n",
        "The model is initialized with pre-trained ImageNet weights.\n",
        "The top (fully connected) layers are removed to allow customization.\n",
        "The input size is set to (350x350x3), matching the dataset resolution."
      ],
      "metadata": {
        "id": "ahXkf-0duD-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(350, 350, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkmCrNQSuZXW",
        "outputId": "03944a73-0638-4c38-d55c-6aa6eeb92b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **↪** **Freeze Base Model Layers:** Freezing prevents the pre-trained convolutional layers from being updated. This ensures the model retains the general feature extraction capabilities.Only the newly added classifier layers will be trained"
      ],
      "metadata": {
        "id": "Gqotuwuhua01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "LBDOMwJuums1",
        "outputId": "ed4ce98a-eb97-4308-c18d-078b2d5d2c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'base_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0516355ef7a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'base_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **Define New Model:**\n",
        "A Flatten layer converts extracted features into a 1D vector\n",
        "A fully connected Dense layer with 256 neurons and ReLU activation is added\n",
        "The final output layer has a single neuron with sigmoid activation for binary classification"
      ],
      "metadata": {
        "id": "FXWUbAV8uqC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification: wildfire vs. no wildfire\n",
        "])"
      ],
      "metadata": {
        "id": "Q0bq5I8AuzIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **Compile Model**\n",
        "Adam optimizer is used for efficient training\n",
        "Binary cross-entropy is the loss function, suitable for binary classification\n",
        "Accuracy is used as the evaluation metric"
      ],
      "metadata": {
        "id": "AA0d-kPvu5yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "mEphLIXjvALO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Model**"
      ],
      "metadata": {
        "id": "4SVm2_NMvXR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained using augmented wildfire images, with validation data used to monitor performance. Early stopping and learning rate adjustments are applied to optimize training efficiency."
      ],
      "metadata": {
        "id": "QU9MYXyrvdQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **↪** **Define Callbacks:**\n",
        "- EarlyStopping: Stops training if validation loss does not improve for 3 epochs, restoring the best model weights to prevent overfitting ReduceLROnPlateau: Reduces learning rate by a factor of 0.5 if validation loss\n",
        "stagnates for 2 epochs, ensuring better convergence."
      ],
      "metadata": {
        "id": "nZnUBNWOvsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "ib31DYknwQ1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**↪** **Train the Model:**\n",
        " - The model is trained for 20 epochs using the preprocessed dataset. Training data is fed using an augmented ImageDataGenerator. The validation dataset is used to evaluate generalization. Callbacks ensure efficient training by preventing overfitting and adjusting learning rates."
      ],
      "metadata": {
        "id": "gJADImvEv7f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "id": "2Eswcf_bwaLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **↪** **Plot Training and Validation Accuracy:**\n",
        "- Accuracy curves are plotted to visualize model performance.\n",
        "The trend helps identify overfitting or underfitting issues."
      ],
      "metadata": {
        "id": "f4x5z01EwdEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GJXezu38wnD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE TUNING THE MODEL**"
      ],
      "metadata": {
        "id": "JWNL5Tz5xVYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **↪** **Unfreeze base model layers for further training. Use a lower learning rate for gradual adjustments**"
      ],
      "metadata": {
        "id": "dkRdyTGTxv1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "\n",
        "# Recompile with lower LR\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=config['lr']),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune for 10 epochs\n",
        "history_ft = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history_ft.history['accuracy'], label='Training Accuracy (Fine-tune)')\n",
        "plt.plot(history_ft.history['val_accuracy'], label='Validation Accuracy (Fine-tune)')\n",
        "plt.legend()\n",
        "plt.title('Fine-Tuning Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8oa3_zixx0jF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}